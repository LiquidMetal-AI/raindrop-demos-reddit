/** Input parameters for image classification tasks
 * @remarks
 * Used to provide image data for classification models that can identify and categorize image content.
 */
export type AiImageClassificationInput = {
  /** Raw image data as byte array in standard formats (JPEG, PNG, etc.) */
  image: number[];
};

/** Output from image classification
 * @remarks
 * Returns an array of classification results, each containing a confidence score and label.
 * Multiple results allow for cases where an image might belong to several categories.
 */
export type AiImageClassificationOutput = {
  /** Confidence score between 0 and 1, where 1 indicates highest confidence */
  score?: number;
  /** Classified label/category from the model's training taxonomy */
  label?: string;
}[];

/** Input parameters for image-to-text conversion
 * @remarks
 * Configures the conversion of images to descriptive text using multimodal models.
 * Supports various generation parameters to control the output quality and style.
 */
export type AiImageToTextInput = {
  /** Raw image data as byte array in standard formats (JPEG, PNG, etc.) */
  image: number[];
  /** Optional prompt to guide the image description generation
   * @example "Describe the main objects and their relationships in this image"
   */
  prompt?: string;
  /** Maximum number of tokens to generate in the response
   * @remarks Helps control response length and computation time
   */
  max_tokens?: number;
  /** Sampling temperature (0-1)
   * @remarks Higher values (e.g., 0.8) make output more creative, lower values (e.g., 0.2) more focused
   */
  temperature?: number;
  /** Top-p (nucleus) sampling parameter
   * @remarks Controls diversity by limiting cumulative probability of considered tokens
   */
  top_p?: number;
  /** Top-k sampling parameter
   * @remarks Limits the number of tokens considered for each generation step
   */
  top_k?: number;
  /** Random seed for reproducibility
   * @remarks Same seed with same input produces identical output
   */
  seed?: number;
  /** Penalty for token repetition
   * @remarks Higher values reduce word and phrase repetition
   */
  repetition_penalty?: number;
  /** Penalty for frequency of token usage
   * @remarks Reduces use of common tokens for more diverse output
   */
  frequency_penalty?: number;
  /** Penalty for presence of tokens
   * @remarks Reduces reuse of concepts already mentioned
   */
  presence_penalty?: number;
  /** Whether to return raw model output without post-processing */
  raw?: boolean;
  /** Optional chat context messages for conversational image understanding
   * @remarks Enables multi-turn interactions about the image
   */
  messages?: RoleScopedChatInput[];
};

/** Output from image-to-text conversion */
export type AiImageToTextOutput = {
  /** Generated text description of the image */
  description: string;
};

/** Input parameters for object detection
 * @remarks
 * Used to provide image data for models that can identify and locate objects within images.
 * Supports standard image formats like JPEG and PNG.
 */
export type AiObjectDetectionInput = {
  /** Raw image data as byte array in standard formats (JPEG, PNG, etc.) */
  image: number[];
};

/** Output from object detection
 * @remarks
 * Returns an array of detected objects, each with a confidence score and label.
 * Multiple detections allow for identifying several objects in a single image.
 */
export type AiObjectDetectionOutput = {
  /** Confidence score between 0 and 1, where 1 indicates highest confidence */
  score?: number;
  /** Detected object label/category from the model's training taxonomy */
  label?: string;
}[];

/** Input parameters for sentence similarity comparison
 * @remarks
 * Used to compute semantic similarity between a source sentence and multiple comparison sentences.
 * Typically uses embedding-based comparison methods.
 */
export type AiSentenceSimilarityInput = {
  /** Source sentence to compare against - the reference text */
  source: string;
  /** Array of sentences to compare with source
   * @remarks Each sentence will receive a similarity score relative to the source
   */
  sentences: string[];
};

/** Output from sentence similarity comparison as array of similarity scores
 * @remarks
 * Returns an array of floating-point numbers between 0 and 1,
 * where 1 indicates perfect similarity and 0 indicates complete dissimilarity.
 * The array indices correspond to the input sentences array.
 */
export type AiSentenceSimilarityOutput = number[];

/** Input parameters for speech recognition
 * @remarks
 * Used to convert spoken audio into text using automatic speech recognition (ASR) models.
 * Supports common audio formats like WAV, MP3, etc.
 */
export type AiSpeechRecognitionInput = {
  /** Raw audio data as byte array in standard audio formats (WAV, MP3, etc.) */
  audio: number[];
};

/** Output from speech recognition
 * @remarks
 * Provides transcribed text along with optional timing information and subtitle formatting.
 * Word-level timing enables precise audio-text alignment for applications like subtitling.
 */
export type AiSpeechRecognitionOutput = {
  /** Transcribed text from the audio input */
  text?: string;
  /** Word-level timing information for precise alignment
   * @remarks Useful for applications requiring word-level synchronization
   */
  words?: {
    /** Individual transcribed word */
    word: string;
    /** Start time of the word in seconds from audio beginning */
    start: number;
    /** End time of the word in seconds from audio beginning */
    end: number;
  }[];
  /** WebVTT format subtitles for video captioning
   * @remarks Ready-to-use subtitle format compatible with HTML5 video
   */
  vtt?: string;
};

/** Input parameters for text summarization
 * @remarks
 * Configures the summarization of long text into a shorter, coherent version
 * while preserving key information and meaning.
 */
export type AiSummarizationInput = {
  /** Text to be summarized - can be articles, documents, or other long-form content */
  input_text: string;
  /** Maximum length of the generated summary in tokens
   * @remarks Helps control summary length while maintaining coherence
   */
  max_length?: number;
};

/** Output from text summarization
 * @remarks
 * Contains the generated summary that captures the essential information
 * from the input text in a condensed form.
 */
export type AiSummarizationOutput = {
  /** Generated summary text that maintains key points while being more concise */
  summary: string;
};

/** Input parameters for text classification */
export type AiTextClassificationInput = {
  /** Text to be classified */
  text: string;
};

/** Output from text classification */
export type AiTextClassificationOutput = {
  /** Confidence score between 0 and 1 */
  score?: number;
  /** Classification label/category */
  label?: string;
}[];

/** Input parameters for text embeddings generation */
export type AiTextEmbeddingsInput = {
  /** Text or array of texts to generate embeddings for */
  text: string | string[];
  /** Pooling method for the embeddings */
  pooling?: 'cls' | 'mean';
};

/** Output from text embeddings generation */
export type AiTextEmbeddingsOutput = {
  /** Shape of the embedding tensor */
  shape: number[];
  /** Matrix of embedding vectors */
  data: number[][];
};

/** Input structure for role-based chat messages */
export type RoleScopedChatInput = {
  /** Role of the message sender - can be user, assistant, system, or tool */
  role: 'user' | 'assistant' | 'system' | 'tool';
  /** Content of the chat message */
  content: string;
};
/** Configuration for function-calling capabilities in text generation */
export type AiTextGenerationToolInput = {
  /** Type of tool - currently only supports 'function' */
  type: 'function';
  /** Function definition */
  function: {
    /** Name of the function */
    name: string;
    /** Description of what the function does */
    description: string;
    /** Schema for function parameters */
    parameters?: {
      /** Must be 'object' for JSON schema compatibility */
      type: 'object';
      /** Object describing each parameter */
      properties: {
        [key: string]: {
          /** Data type of the parameter */
          type: string;
          /** Description of the parameter */
          description?: string;
        };
      };
      /** Array of required parameter names */
      required: string[];
    };
  };
};
/** Input parameters for text generation */
export type AiTextGenerationInput = {
  /** Initial prompt to guide text generation */
  prompt?: string;
  /** Whether to return raw model output */
  raw?: boolean;
  /** Whether to stream the response */
  stream?: boolean;
  /** Maximum number of tokens to generate */
  max_tokens?: number;
  /** Sampling temperature (0-1) */
  temperature?: number;
  /** Top-p sampling parameter */
  top_p?: number;
  /** Top-k sampling parameter */
  top_k?: number;
  /** Random seed for reproducibility */
  seed?: number;
  /** Penalty for token repetition */
  repetition_penalty?: number;
  /** Penalty for frequency of token usage */
  frequency_penalty?: number;
  /** Penalty for presence of tokens */
  presence_penalty?: number;
  /** Array of chat messages for context */
  messages?: RoleScopedChatInput[];
  /** Array of tool definitions for function calling */
  tools?: AiTextGenerationToolInput[];
};
/** Output from text generation, either a response object or stream */
export type AiTextGenerationOutput =
  | {
      /** Generated text response */
      response?: string;
      /** Array of tool/function calls made during generation */
      tool_calls?: {
        /** Name of the called function */
        name: string;
        /** Arguments passed to the function */
        arguments: unknown;
      }[];
    }
  | ReadableStream;
/** Input parameters for text-to-image generation */
export type AiTextToImageInput = {
  /** Text prompt describing the desired image */
  prompt: string;
  /** Optional input image for img2img or inpainting (as byte array) */
  image?: number[];
  /** Optional mask for inpainting (as byte array) */
  mask?: number[];
  /** Number of denoising steps */
  num_steps?: number;
  /** Strength of conditioning for img2img (0-1) */
  strength?: number;
  /** Classifier-free guidance scale */
  guidance?: number;
};
/** Output from text-to-image generation as raw image bytes */
export type AiTextToImageOutput = ReadableStream<Uint8Array>;
/** Input parameters for text translation */
export type AiTranslationInput = {
  /** Text to be translated */
  text: string;
  /** Target language code */
  target_lang: string;
  /** Optional source language code. If not provided, will be auto-detected */
  source_lang?: string;
};
/** Output from text translation */
export type AiTranslationOutput = {
  /** Translated text in target language */
  translated_text?: string;
};
/** Configuration options for AI gateway requests */
export type GatewayOptions = {
  /** Unique identifier for the request */
  id: string;
  /** Time-to-live in seconds for cache entries */
  cacheTtl?: number;
  /** Whether to bypass cache for this request */
  skipCache?: boolean;
  /** Additional metadata to attach to the request */
  metadata?: Record<string, number | string | boolean | null | bigint>;
};
/** General options for AI operations */
export type AiOptions = {
  /** Gateway-specific configuration */
  gateway?: GatewayOptions;
  /** URL prefix for API endpoints */
  prefix?: string;
  /** Additional HTTP headers to include */
  extraHeaders?: object;
};
/** Text classification models array */
export const TEXT_CLASSIFICATION_MODELS = [
  /** DistilBERT model fine-tuned on SST-2 dataset, quantized to int8 */
  '@cf/huggingface/distilbert-sst-2-int8',
] as const;

/** Text-to-image generation models array */
export const TEXT_TO_IMAGE_MODELS = [
  /** Stability AI's SDXL 1.0 base model for high-quality image generation */
  '@cf/stabilityai/stable-diffusion-xl-base-1.0',
  /** Runway's SD 1.5 model specialized for image inpainting */
  '@cf/runwayml/stable-diffusion-v1-5-inpainting',
  /** Runway's SD 1.5 model for image-to-image transformation */
  '@cf/runwayml/stable-diffusion-v1-5-img2img',
  /** DreamShaper v8 with Latent Consistency Model for faster inference */
  '@cf/lykon/dreamshaper-8-lcm',
  /** ByteDance's optimized SDXL for faster generation */
  '@cf/bytedance/stable-diffusion-xl-lightning',
] as const;

/** Text embeddings models array */
export const TEXT_EMBEDDINGS_MODELS = [
  /** BGE small English model v1.5 - efficient with good performance */
  '@cf/baai/bge-small-en-v1.5',
  /** BGE base English model v1.5 - balanced size and performance */
  '@cf/baai/bge-base-en-v1.5',
  /** BGE large English model v1.5 - highest quality embeddings */
  '@cf/baai/bge-large-en-v1.5',
] as const;

/** Speech recognition models array */
export const SPEECH_RECOGNITION_MODELS = [
  /** OpenAI's Whisper base model for multilingual speech recognition */
  '@cf/openai/whisper',
  /** Tiny English-specific Whisper model for efficient processing */
  '@cf/openai/whisper-tiny-en',
  /** Sherpa-optimized Whisper variant */
  '@cf/openai/whisper-sherpa',
] as const;

/** Image classification models array */
export const IMAGE_CLASSIFICATION_MODELS = [
  /** Microsoft's ResNet-50 model for general image classification */
  '@cf/microsoft/resnet-50',
] as const;

/** Object detection models array */
export const OBJECT_DETECTION_MODELS = [
  /** Facebook's DETR (Detection Transformer) with ResNet-50 backbone */
  '@cf/facebook/detr-resnet-50',
] as const;

/** Available models for text classification tasks */
export type BaseAiTextClassificationModels = (typeof TEXT_CLASSIFICATION_MODELS)[number];
/** Available models for text-to-image generation */
export type BaseAiTextToImageModels = (typeof TEXT_TO_IMAGE_MODELS)[number];
/** Available models for text embeddings generation */
export type BaseAiTextEmbeddingsModels = (typeof TEXT_EMBEDDINGS_MODELS)[number];
/** Available models for speech recognition */
export type BaseAiSpeechRecognitionModels = (typeof SPEECH_RECOGNITION_MODELS)[number];
/** Available models for image classification */
export type BaseAiImageClassificationModels = (typeof IMAGE_CLASSIFICATION_MODELS)[number];
/** Available models for object detection in images */
export type BaseAiObjectDetectionModels = (typeof OBJECT_DETECTION_MODELS)[number];
/** Text generation models array */
export const TEXT_GENERATION_MODELS = [
  // Meta's LLaMA family
  /** LLaMA 3.1 8B instruction-tuned model */
  '@cf/meta/llama-3.1-8b-instruct',
  /** LLaMA 3 8B instruction-tuned model */
  '@cf/meta/llama-3-8b-instruct',
  /** LLaMA 3 8B instruction model with AWQ quantization */
  '@cf/meta/llama-3-8b-instruct-awq',
  /** LLaMA 2 7B chat model with INT8 quantization */
  '@cf/meta/llama-2-7b-chat-int8',
  /** LLaMA 2 7B chat model in FP16 format */
  '@cf/meta/llama-2-7b-chat-fp16',
  /** LLaMA 2 7B chat model with LoRA fine-tuning */
  '@cf/meta-llama/llama-2-7b-chat-hf-lora',
  /** LLaMA 3.1 70B instruction-tuned model */
  '@cf/meta/llama-3.1-70b-instruct',
  /** LLaMA 3.3 70B instruction-tuned model */
  '@cf/meta/llama-3.3-70b-instruct-fp8-fast',

  // Mistral AI models
  /** Mistral 7B instruction model v0.1 */
  '@cf/mistral/mistral-7b-instruct-v0.1',
  /** Mistral 7B instruction model v0.2 with LoRA */
  '@cf/mistral/mistral-7b-instruct-v0.2-lora',
  /** Mistral 7B instruction model v0.2 */
  '@hf/mistral/mistral-7b-instruct-v0.2',

  // TheBloke's optimized models
  /** LLaMA 2 13B chat model with AWQ quantization */
  '@hf/thebloke/llama-2-13b-chat-awq',
  /** Zephyr 7B beta model with AWQ quantization */
  '@hf/thebloke/zephyr-7b-beta-awq',
  /** Mistral 7B instruction v0.1 with AWQ */
  '@hf/thebloke/mistral-7b-instruct-v0.1-awq',
  /** CodeLLaMA 7B instruction model with AWQ */
  '@hf/thebloke/codellama-7b-instruct-awq',
  /** OpenHermes 2.5 Mistral 7B with AWQ */
  '@hf/thebloke/openhermes-2.5-mistral-7b-awq',
  /** NeuralChat 7B v3.1 with AWQ */
  '@hf/thebloke/neural-chat-7b-v3-1-awq',
  /** LLamaGuard 7B safety model with AWQ */
  '@hf/thebloke/llamaguard-7b-awq',
  /** DeepSeek Coder 6.7B base with AWQ */
  '@hf/thebloke/deepseek-coder-6.7b-base-awq',
  /** DeepSeek Coder 6.7B instruction with AWQ */
  '@hf/thebloke/deepseek-coder-6.7b-instruct-awq',
  /** German DiscoLM 7B v1 with AWQ */
  '@cf/thebloke/discolm-german-7b-v1-awq',

  // Google's Gemma models
  /** Gemma 7B instruction-tuned model */
  '@hf/google/gemma-7b-it',
  /** Gemma 2B instruction model with LoRA */
  '@cf/google/gemma-2b-it-lora',
  /** Gemma 7B instruction model with LoRA */
  '@cf/google/gemma-7b-it-lora',

  // Specialized models
  /** DeepSeek Math 7B instruction model */
  '@cf/deepseek-ai/deepseek-math-7b-instruct',
  /** SQLCoder 7B v2 for SQL generation */
  '@cf/defog/sqlcoder-7b-2',
  /** OpenChat 3.5 model (January 2024) */
  '@cf/openchat/openchat-3.5-0106',
  /** Falcon 7B instruction model */
  '@cf/tiiuae/falcon-7b-instruct',
  /** Hermes 2 Pro Mistral 7B */
  '@hf/nousresearch/hermes-2-pro-mistral-7b',
  /** Starling LM 7B beta */
  '@hf/nexusflow/starling-lm-7b-beta',

  // Qwen models
  /** Qwen 1.5 0.5B chat model */
  '@cf/qwen/qwen1.5-0.5b-chat',
  /** Qwen 1.5 1.8B chat model */
  '@cf/qwen/qwen1.5-1.8b-chat',
  /** Qwen 1.5 7B chat with AWQ */
  '@cf/qwen/qwen1.5-7b-chat-awq',
  /** Qwen 1.5 14B chat with AWQ */
  '@cf/qwen/qwen1.5-14b-chat-awq',

  // Other models
  /** TinyLLaMA 1.1B chat model v1.0 */
  '@cf/tinyllama/tinyllama-1.1b-chat-v1.0',
  /** Microsoft Phi-2 model */
  '@cf/microsoft/phi-2',
  /** Una Cybertron 7B v2 in BF16 format */
  '@cf/fblgit/una-cybertron-7b-v2-bf16',
  /** Una Cybertron 7B v2 with AWQ */
  '@cf/fblgit/una-cybertron-7b-v2-awq',
] as const;

/** Text translation models array */
export const TRANSLATION_MODELS = [
  /** Meta's Many-to-Many multilingual translation model (1.2B parameters) */
  '@cf/meta/m2m100-1.2b',
] as const;

/** Text summarization models array */
export const SUMMARIZATION_MODELS = [
  /** Facebook's BART model fine-tuned on CNN news articles for summarization */
  '@cf/facebook/bart-large-cnn',
] as const;

/** Image-to-text generation models array */
export const IMAGE_TO_TEXT_MODELS = [
  /** Unum's UForm Gen2 model based on Qwen architecture (500M parameters) */
  '@cf/unum/uform-gen2-qwen-500m',
  /** LLaVA 1.5 multimodal model for image understanding and generation */
  '@cf/llava-hf/llava-1.5-7b-hf',
] as const;

/** Available models for text generation and chat */
export type BaseAiTextGenerationModels = (typeof TEXT_GENERATION_MODELS)[number];
/** Available models for text translation */
export type BaseAiTranslationModels = (typeof TRANSLATION_MODELS)[number];
/** Available models for text summarization */
export type BaseAiSummarizationModels = (typeof SUMMARIZATION_MODELS)[number];
/** Available models for image-to-text generation */
export type BaseAiImageToTextModels = (typeof IMAGE_TO_TEXT_MODELS)[number];
/** Main interface for AI operations
 * @remarks
 * Provides a unified interface for running various AI models across different tasks.
 * Each model type has its own specific input and output types.
 */
export interface Ai {
  /** Run text classification models
   * @param model - The text classification model to use
   * @param inputs - Text input to classify
   * @param options - Optional configuration for the request
   * @returns Promise resolving to classification results with labels and confidence scores
   */
  run(
    model: BaseAiTextClassificationModels,
    inputs: AiTextClassificationInput,
    options?: AiOptions,
  ): Promise<AiTextClassificationOutput>;
  /** Run text-to-image generation models
   * @param model - The text-to-image model to use
   * @param inputs - Text prompt and optional image/mask for img2img or inpainting
   * @param options - Optional configuration for the request
   * @returns Promise resolving to generated image as byte array
   */
  run(model: BaseAiTextToImageModels, inputs: AiTextToImageInput, options?: AiOptions): Promise<AiTextToImageOutput>;
  /** Run text embedding models
   * @param model - The text embedding model to use
   * @param inputs - Text or array of texts to embed
   * @param options - Optional configuration for the request
   * @returns Promise resolving to embedding vectors with shape information
   */
  run(
    model: BaseAiTextEmbeddingsModels,
    inputs: AiTextEmbeddingsInput,
    options?: AiOptions,
  ): Promise<AiTextEmbeddingsOutput>;
  /** Run speech recognition models
   * @param model - The speech recognition model to use
   * @param inputs - Audio data to transcribe
   * @param options - Optional configuration for the request
   * @returns Promise resolving to transcription with optional timing and subtitle data
   */
  run(
    model: BaseAiSpeechRecognitionModels,
    inputs: AiSpeechRecognitionInput,
    options?: AiOptions,
  ): Promise<AiSpeechRecognitionOutput>;
  /** Run image classification models
   * @param model - The image classification model to use
   * @param inputs - Image data to classify
   * @param options - Optional configuration for the request
   * @returns Promise resolving to classification results with labels and confidence scores
   */
  run(
    model: BaseAiImageClassificationModels,
    inputs: AiImageClassificationInput,
    options?: AiOptions,
  ): Promise<AiImageClassificationOutput>;
  /** Run object detection models
   * @param model - The object detection model to use
   * @param inputs - Image data to analyze for objects
   * @param options - Optional configuration for the request
   * @returns Promise resolving to detection results with object labels and confidence scores
   */
  run(
    model: BaseAiObjectDetectionModels,
    inputs: AiObjectDetectionInput,
    options?: AiOptions,
  ): Promise<AiObjectDetectionOutput>;
  /** Run text generation and chat models
   * @param model - The text generation model to use
   * @param inputs - Prompt, chat messages, and generation parameters
   * @param options - Optional configuration for the request
   * @returns Promise resolving to generated text response or stream
   */
  run(
    model: BaseAiTextGenerationModels,
    inputs: AiTextGenerationInput,
    options?: AiOptions,
  ): Promise<AiTextGenerationOutput>;
  /** Run text translation models
   * @param model - The translation model to use
   * @param inputs - Text to translate and language codes
   * @param options - Optional configuration for the request
   * @returns Promise resolving to translated text
   */
  run(model: BaseAiTranslationModels, inputs: AiTranslationInput, options?: AiOptions): Promise<AiTranslationOutput>;
  /** Run text summarization models
   * @param model - The summarization model to use
   * @param inputs - Text to summarize and length parameters
   * @param options - Optional configuration for the request
   * @returns Promise resolving to generated summary text
   */
  run(
    model: BaseAiSummarizationModels,
    inputs: AiSummarizationInput,
    options?: AiOptions,
  ): Promise<AiSummarizationOutput>;
  /** Run image-to-text generation models
   * @param model - The image-to-text model to use
   * @param inputs - Image data and optional generation parameters
   * @param options - Optional configuration for the request
   * @returns Promise resolving to generated text description of the image
   */
  run(model: BaseAiImageToTextModels, inputs: AiImageToTextInput, options?: AiOptions): Promise<AiImageToTextOutput>;
}
