/** Input parameters for image classification tasks
 * @remarks
 * Used to provide image data for classification models that can identify and categorize image content.
 */
export type AiImageClassificationInput = {
    /** Raw image data as byte array in standard formats (JPEG, PNG, etc.) */
    image: number[];
};
/** Output from image classification
 * @remarks
 * Returns an array of classification results, each containing a confidence score and label.
 * Multiple results allow for cases where an image might belong to several categories.
 */
export type AiImageClassificationOutput = {
    /** Confidence score between 0 and 1, where 1 indicates highest confidence */
    score?: number;
    /** Classified label/category from the model's training taxonomy */
    label?: string;
}[];
/** Input parameters for image-to-text conversion
 * @remarks
 * Configures the conversion of images to descriptive text using multimodal models.
 * Supports various generation parameters to control the output quality and style.
 */
export type AiImageToTextInput = {
    /** Raw image data as byte array in standard formats (JPEG, PNG, etc.) */
    image: number[];
    /** Optional prompt to guide the image description generation
     * @example "Describe the main objects and their relationships in this image"
     */
    prompt?: string;
    /** Maximum number of tokens to generate in the response
     * @remarks Helps control response length and computation time
     */
    max_tokens?: number;
    /** Sampling temperature (0-1)
     * @remarks Higher values (e.g., 0.8) make output more creative, lower values (e.g., 0.2) more focused
     */
    temperature?: number;
    /** Top-p (nucleus) sampling parameter
     * @remarks Controls diversity by limiting cumulative probability of considered tokens
     */
    top_p?: number;
    /** Top-k sampling parameter
     * @remarks Limits the number of tokens considered for each generation step
     */
    top_k?: number;
    /** Random seed for reproducibility
     * @remarks Same seed with same input produces identical output
     */
    seed?: number;
    /** Penalty for token repetition
     * @remarks Higher values reduce word and phrase repetition
     */
    repetition_penalty?: number;
    /** Penalty for frequency of token usage
     * @remarks Reduces use of common tokens for more diverse output
     */
    frequency_penalty?: number;
    /** Penalty for presence of tokens
     * @remarks Reduces reuse of concepts already mentioned
     */
    presence_penalty?: number;
    /** Whether to return raw model output without post-processing */
    raw?: boolean;
    /** Optional chat context messages for conversational image understanding
     * @remarks Enables multi-turn interactions about the image
     */
    messages?: RoleScopedChatInput[];
};
/** Output from image-to-text conversion */
export type AiImageToTextOutput = {
    /** Generated text description of the image */
    description: string;
};
/** Input parameters for object detection
 * @remarks
 * Used to provide image data for models that can identify and locate objects within images.
 * Supports standard image formats like JPEG and PNG.
 */
export type AiObjectDetectionInput = {
    /** Raw image data as byte array in standard formats (JPEG, PNG, etc.) */
    image: number[];
};
/** Output from object detection
 * @remarks
 * Returns an array of detected objects, each with a confidence score and label.
 * Multiple detections allow for identifying several objects in a single image.
 */
export type AiObjectDetectionOutput = {
    /** Confidence score between 0 and 1, where 1 indicates highest confidence */
    score?: number;
    /** Detected object label/category from the model's training taxonomy */
    label?: string;
}[];
/** Input parameters for sentence similarity comparison
 * @remarks
 * Used to compute semantic similarity between a source sentence and multiple comparison sentences.
 * Typically uses embedding-based comparison methods.
 */
export type AiSentenceSimilarityInput = {
    /** Source sentence to compare against - the reference text */
    source: string;
    /** Array of sentences to compare with source
     * @remarks Each sentence will receive a similarity score relative to the source
     */
    sentences: string[];
};
/** Output from sentence similarity comparison as array of similarity scores
 * @remarks
 * Returns an array of floating-point numbers between 0 and 1,
 * where 1 indicates perfect similarity and 0 indicates complete dissimilarity.
 * The array indices correspond to the input sentences array.
 */
export type AiSentenceSimilarityOutput = number[];
/** Input parameters for speech recognition
 * @remarks
 * Used to convert spoken audio into text using automatic speech recognition (ASR) models.
 * Supports common audio formats like WAV, MP3, etc.
 */
export type AiSpeechRecognitionInput = {
    /** Raw audio data as byte array in standard audio formats (WAV, MP3, etc.) */
    audio: number[];
};
/** Output from speech recognition
 * @remarks
 * Provides transcribed text along with optional timing information and subtitle formatting.
 * Word-level timing enables precise audio-text alignment for applications like subtitling.
 */
export type AiSpeechRecognitionOutput = {
    /** Transcribed text from the audio input */
    text?: string;
    /** Word-level timing information for precise alignment
     * @remarks Useful for applications requiring word-level synchronization
     */
    words?: {
        /** Individual transcribed word */
        word: string;
        /** Start time of the word in seconds from audio beginning */
        start: number;
        /** End time of the word in seconds from audio beginning */
        end: number;
    }[];
    /** WebVTT format subtitles for video captioning
     * @remarks Ready-to-use subtitle format compatible with HTML5 video
     */
    vtt?: string;
};
/** Input parameters for text summarization
 * @remarks
 * Configures the summarization of long text into a shorter, coherent version
 * while preserving key information and meaning.
 */
export type AiSummarizationInput = {
    /** Text to be summarized - can be articles, documents, or other long-form content */
    input_text: string;
    /** Maximum length of the generated summary in tokens
     * @remarks Helps control summary length while maintaining coherence
     */
    max_length?: number;
};
/** Output from text summarization
 * @remarks
 * Contains the generated summary that captures the essential information
 * from the input text in a condensed form.
 */
export type AiSummarizationOutput = {
    /** Generated summary text that maintains key points while being more concise */
    summary: string;
};
/** Input parameters for text classification */
export type AiTextClassificationInput = {
    /** Text to be classified */
    text: string;
};
/** Output from text classification */
export type AiTextClassificationOutput = {
    /** Confidence score between 0 and 1 */
    score?: number;
    /** Classification label/category */
    label?: string;
}[];
/** Input parameters for text embeddings generation */
export type AiTextEmbeddingsInput = {
    /** Text or array of texts to generate embeddings for */
    text: string | string[];
    /** Pooling method for the embeddings */
    pooling?: 'cls' | 'mean';
};
/** Output from text embeddings generation */
export type AiTextEmbeddingsOutput = {
    /** Shape of the embedding tensor */
    shape: number[];
    /** Matrix of embedding vectors */
    data: number[][];
};
/** Input structure for role-based chat messages */
export type RoleScopedChatInput = {
    /** Role of the message sender - can be user, assistant, system, or tool */
    role: 'user' | 'assistant' | 'system' | 'tool';
    /** Content of the chat message */
    content: string;
};
/** Configuration for function-calling capabilities in text generation */
export type AiTextGenerationToolInput = {
    /** Type of tool - currently only supports 'function' */
    type: 'function';
    /** Function definition */
    function: {
        /** Name of the function */
        name: string;
        /** Description of what the function does */
        description: string;
        /** Schema for function parameters */
        parameters?: {
            /** Must be 'object' for JSON schema compatibility */
            type: 'object';
            /** Object describing each parameter */
            properties: {
                [key: string]: {
                    /** Data type of the parameter */
                    type: string;
                    /** Description of the parameter */
                    description?: string;
                };
            };
            /** Array of required parameter names */
            required: string[];
        };
    };
};
/** Input parameters for text generation */
export type AiTextGenerationInput = {
    /** Initial prompt to guide text generation */
    prompt?: string;
    /** Whether to return raw model output */
    raw?: boolean;
    /** Whether to stream the response */
    stream?: boolean;
    /** Maximum number of tokens to generate */
    max_tokens?: number;
    /** Sampling temperature (0-1) */
    temperature?: number;
    /** Top-p sampling parameter */
    top_p?: number;
    /** Top-k sampling parameter */
    top_k?: number;
    /** Random seed for reproducibility */
    seed?: number;
    /** Penalty for token repetition */
    repetition_penalty?: number;
    /** Penalty for frequency of token usage */
    frequency_penalty?: number;
    /** Penalty for presence of tokens */
    presence_penalty?: number;
    /** Array of chat messages for context */
    messages?: RoleScopedChatInput[];
    /** Array of tool definitions for function calling */
    tools?: AiTextGenerationToolInput[];
};
/** Output from text generation, either a response object or stream */
export type AiTextGenerationOutput = {
    /** Generated text response */
    response?: string;
    /** Array of tool/function calls made during generation */
    tool_calls?: {
        /** Name of the called function */
        name: string;
        /** Arguments passed to the function */
        arguments: unknown;
    }[];
} | ReadableStream;
/** Input parameters for text-to-image generation */
export type AiTextToImageInput = {
    /** Text prompt describing the desired image */
    prompt: string;
    /** Optional input image for img2img or inpainting (as byte array) */
    image?: number[];
    /** Optional mask for inpainting (as byte array) */
    mask?: number[];
    /** Number of denoising steps */
    num_steps?: number;
    /** Strength of conditioning for img2img (0-1) */
    strength?: number;
    /** Classifier-free guidance scale */
    guidance?: number;
};
/** Output from text-to-image generation as raw image bytes */
export type AiTextToImageOutput = ReadableStream<Uint8Array>;
/** Input parameters for text translation */
export type AiTranslationInput = {
    /** Text to be translated */
    text: string;
    /** Target language code */
    target_lang: string;
    /** Optional source language code. If not provided, will be auto-detected */
    source_lang?: string;
};
/** Output from text translation */
export type AiTranslationOutput = {
    /** Translated text in target language */
    translated_text?: string;
};
/** Configuration options for AI gateway requests */
export type GatewayOptions = {
    /** Unique identifier for the request */
    id: string;
    /** Time-to-live in seconds for cache entries */
    cacheTtl?: number;
    /** Whether to bypass cache for this request */
    skipCache?: boolean;
    /** Additional metadata to attach to the request */
    metadata?: Record<string, number | string | boolean | null | bigint>;
};
/** General options for AI operations */
export type AiOptions = {
    /** Gateway-specific configuration */
    gateway?: GatewayOptions;
    /** URL prefix for API endpoints */
    prefix?: string;
    /** Additional HTTP headers to include */
    extraHeaders?: object;
};
/** Text classification models array */
export declare const TEXT_CLASSIFICATION_MODELS: readonly ["@cf/huggingface/distilbert-sst-2-int8"];
/** Text-to-image generation models array */
export declare const TEXT_TO_IMAGE_MODELS: readonly ["@cf/stabilityai/stable-diffusion-xl-base-1.0", "@cf/runwayml/stable-diffusion-v1-5-inpainting", "@cf/runwayml/stable-diffusion-v1-5-img2img", "@cf/lykon/dreamshaper-8-lcm", "@cf/bytedance/stable-diffusion-xl-lightning"];
/** Text embeddings models array */
export declare const TEXT_EMBEDDINGS_MODELS: readonly ["@cf/baai/bge-small-en-v1.5", "@cf/baai/bge-base-en-v1.5", "@cf/baai/bge-large-en-v1.5"];
/** Speech recognition models array */
export declare const SPEECH_RECOGNITION_MODELS: readonly ["@cf/openai/whisper", "@cf/openai/whisper-tiny-en", "@cf/openai/whisper-sherpa"];
/** Image classification models array */
export declare const IMAGE_CLASSIFICATION_MODELS: readonly ["@cf/microsoft/resnet-50"];
/** Object detection models array */
export declare const OBJECT_DETECTION_MODELS: readonly ["@cf/facebook/detr-resnet-50"];
/** Available models for text classification tasks */
export type BaseAiTextClassificationModels = (typeof TEXT_CLASSIFICATION_MODELS)[number];
/** Available models for text-to-image generation */
export type BaseAiTextToImageModels = (typeof TEXT_TO_IMAGE_MODELS)[number];
/** Available models for text embeddings generation */
export type BaseAiTextEmbeddingsModels = (typeof TEXT_EMBEDDINGS_MODELS)[number];
/** Available models for speech recognition */
export type BaseAiSpeechRecognitionModels = (typeof SPEECH_RECOGNITION_MODELS)[number];
/** Available models for image classification */
export type BaseAiImageClassificationModels = (typeof IMAGE_CLASSIFICATION_MODELS)[number];
/** Available models for object detection in images */
export type BaseAiObjectDetectionModels = (typeof OBJECT_DETECTION_MODELS)[number];
/** Text generation models array */
export declare const TEXT_GENERATION_MODELS: readonly ["@cf/meta/llama-3.1-8b-instruct", "@cf/meta/llama-3-8b-instruct", "@cf/meta/llama-3-8b-instruct-awq", "@cf/meta/llama-2-7b-chat-int8", "@cf/meta/llama-2-7b-chat-fp16", "@cf/meta-llama/llama-2-7b-chat-hf-lora", "@cf/meta/llama-3.1-70b-instruct", "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/mistral/mistral-7b-instruct-v0.1", "@cf/mistral/mistral-7b-instruct-v0.2-lora", "@hf/mistral/mistral-7b-instruct-v0.2", "@hf/thebloke/llama-2-13b-chat-awq", "@hf/thebloke/zephyr-7b-beta-awq", "@hf/thebloke/mistral-7b-instruct-v0.1-awq", "@hf/thebloke/codellama-7b-instruct-awq", "@hf/thebloke/openhermes-2.5-mistral-7b-awq", "@hf/thebloke/neural-chat-7b-v3-1-awq", "@hf/thebloke/llamaguard-7b-awq", "@hf/thebloke/deepseek-coder-6.7b-base-awq", "@hf/thebloke/deepseek-coder-6.7b-instruct-awq", "@cf/thebloke/discolm-german-7b-v1-awq", "@hf/google/gemma-7b-it", "@cf/google/gemma-2b-it-lora", "@cf/google/gemma-7b-it-lora", "@cf/deepseek-ai/deepseek-math-7b-instruct", "@cf/defog/sqlcoder-7b-2", "@cf/openchat/openchat-3.5-0106", "@cf/tiiuae/falcon-7b-instruct", "@hf/nousresearch/hermes-2-pro-mistral-7b", "@hf/nexusflow/starling-lm-7b-beta", "@cf/qwen/qwen1.5-0.5b-chat", "@cf/qwen/qwen1.5-1.8b-chat", "@cf/qwen/qwen1.5-7b-chat-awq", "@cf/qwen/qwen1.5-14b-chat-awq", "@cf/tinyllama/tinyllama-1.1b-chat-v1.0", "@cf/microsoft/phi-2", "@cf/fblgit/una-cybertron-7b-v2-bf16", "@cf/fblgit/una-cybertron-7b-v2-awq"];
/** Text translation models array */
export declare const TRANSLATION_MODELS: readonly ["@cf/meta/m2m100-1.2b"];
/** Text summarization models array */
export declare const SUMMARIZATION_MODELS: readonly ["@cf/facebook/bart-large-cnn"];
/** Image-to-text generation models array */
export declare const IMAGE_TO_TEXT_MODELS: readonly ["@cf/unum/uform-gen2-qwen-500m", "@cf/llava-hf/llava-1.5-7b-hf"];
/** Available models for text generation and chat */
export type BaseAiTextGenerationModels = (typeof TEXT_GENERATION_MODELS)[number];
/** Available models for text translation */
export type BaseAiTranslationModels = (typeof TRANSLATION_MODELS)[number];
/** Available models for text summarization */
export type BaseAiSummarizationModels = (typeof SUMMARIZATION_MODELS)[number];
/** Available models for image-to-text generation */
export type BaseAiImageToTextModels = (typeof IMAGE_TO_TEXT_MODELS)[number];
/** Main interface for AI operations
 * @remarks
 * Provides a unified interface for running various AI models across different tasks.
 * Each model type has its own specific input and output types.
 */
export interface Ai {
    /** Run text classification models
     * @param model - The text classification model to use
     * @param inputs - Text input to classify
     * @param options - Optional configuration for the request
     * @returns Promise resolving to classification results with labels and confidence scores
     */
    run(model: BaseAiTextClassificationModels, inputs: AiTextClassificationInput, options?: AiOptions): Promise<AiTextClassificationOutput>;
    /** Run text-to-image generation models
     * @param model - The text-to-image model to use
     * @param inputs - Text prompt and optional image/mask for img2img or inpainting
     * @param options - Optional configuration for the request
     * @returns Promise resolving to generated image as byte array
     */
    run(model: BaseAiTextToImageModels, inputs: AiTextToImageInput, options?: AiOptions): Promise<AiTextToImageOutput>;
    /** Run text embedding models
     * @param model - The text embedding model to use
     * @param inputs - Text or array of texts to embed
     * @param options - Optional configuration for the request
     * @returns Promise resolving to embedding vectors with shape information
     */
    run(model: BaseAiTextEmbeddingsModels, inputs: AiTextEmbeddingsInput, options?: AiOptions): Promise<AiTextEmbeddingsOutput>;
    /** Run speech recognition models
     * @param model - The speech recognition model to use
     * @param inputs - Audio data to transcribe
     * @param options - Optional configuration for the request
     * @returns Promise resolving to transcription with optional timing and subtitle data
     */
    run(model: BaseAiSpeechRecognitionModels, inputs: AiSpeechRecognitionInput, options?: AiOptions): Promise<AiSpeechRecognitionOutput>;
    /** Run image classification models
     * @param model - The image classification model to use
     * @param inputs - Image data to classify
     * @param options - Optional configuration for the request
     * @returns Promise resolving to classification results with labels and confidence scores
     */
    run(model: BaseAiImageClassificationModels, inputs: AiImageClassificationInput, options?: AiOptions): Promise<AiImageClassificationOutput>;
    /** Run object detection models
     * @param model - The object detection model to use
     * @param inputs - Image data to analyze for objects
     * @param options - Optional configuration for the request
     * @returns Promise resolving to detection results with object labels and confidence scores
     */
    run(model: BaseAiObjectDetectionModels, inputs: AiObjectDetectionInput, options?: AiOptions): Promise<AiObjectDetectionOutput>;
    /** Run text generation and chat models
     * @param model - The text generation model to use
     * @param inputs - Prompt, chat messages, and generation parameters
     * @param options - Optional configuration for the request
     * @returns Promise resolving to generated text response or stream
     */
    run(model: BaseAiTextGenerationModels, inputs: AiTextGenerationInput, options?: AiOptions): Promise<AiTextGenerationOutput>;
    /** Run text translation models
     * @param model - The translation model to use
     * @param inputs - Text to translate and language codes
     * @param options - Optional configuration for the request
     * @returns Promise resolving to translated text
     */
    run(model: BaseAiTranslationModels, inputs: AiTranslationInput, options?: AiOptions): Promise<AiTranslationOutput>;
    /** Run text summarization models
     * @param model - The summarization model to use
     * @param inputs - Text to summarize and length parameters
     * @param options - Optional configuration for the request
     * @returns Promise resolving to generated summary text
     */
    run(model: BaseAiSummarizationModels, inputs: AiSummarizationInput, options?: AiOptions): Promise<AiSummarizationOutput>;
    /** Run image-to-text generation models
     * @param model - The image-to-text model to use
     * @param inputs - Image data and optional generation parameters
     * @param options - Optional configuration for the request
     * @returns Promise resolving to generated text description of the image
     */
    run(model: BaseAiImageToTextModels, inputs: AiImageToTextInput, options?: AiOptions): Promise<AiImageToTextOutput>;
}
//# sourceMappingURL=ai.d.ts.map